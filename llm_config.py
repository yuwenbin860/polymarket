#!/usr/bin/env python3
"""
LLMé…ç½®ç®¡ç†å™¨
=============

æ”¯æŒå¤šä¸ªé¢„è®¾é…ç½®ï¼Œæ–¹ä¾¿åœ¨ä¸åŒæä¾›å•†å’Œæ¨¡å‹é—´å¿«é€Ÿåˆ‡æ¢ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # å‘½ä»¤è¡Œåˆ‡æ¢
    python local_scanner_v2.py --profile siliconflow
    python local_scanner_v2.py --profile deepseek
    python local_scanner_v2.py --profile ollama
    
    # ç¯å¢ƒå˜é‡åˆ‡æ¢
    export LLM_PROFILE=siliconflow
    python local_scanner_v2.py
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®
    python llm_config.py --list
    
    # æµ‹è¯•æŸä¸ªé…ç½®
    python llm_config.py --test siliconflow
    
    # æ·»åŠ æ–°é…ç½®
    python llm_config.py --add
"""

import os
import json
import sys
from dataclasses import dataclass, asdict, field
from typing import Optional, Dict, List
from pathlib import Path


# ============================================================
# åœºæ™¯å¸¸é‡
# ============================================================

class LLMScenario:
    """
    LLMä½¿ç”¨åœºæ™¯å¸¸é‡

    å®šä¹‰ç³»ç»Ÿä¸­ä½¿ç”¨LLMçš„ä¸åŒåœºæ™¯ï¼Œå¯ä»¥ä¸ºæ¯ä¸ªåœºæ™¯é…ç½®ä¸“ç”¨æ¨¡å‹ã€‚
    """
    # Tagåˆ†ç±»åœºæ™¯ - ä½¿ç”¨æ€è€ƒæ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†ç±»
    TAG_CLASSIFICATION = "tag_classification"
    # ç­–ç•¥æ‰«æåœºæ™¯ - ä½¿ç”¨å¿«é€Ÿæ¨¡å‹è¿›è¡Œå¥—åˆ©ç­–ç•¥åˆ†æ
    STRATEGY_SCAN = "strategy_scan"
    # è¯­ä¹‰åˆ†æåœºæ™¯ - ä½¿ç”¨å¿«é€Ÿæ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ
    SEMANTIC_ANALYSIS = "semantic_analysis"
    # å…³ç³»æ£€æµ‹åœºæ™¯ - ä½¿ç”¨æ€è€ƒæ¨¡å‹è¿›è¡Œå¸‚åœºå…³ç³»æ¨ç†
    RELATIONSHIP_DETECTION = "relationship_detection"


# ============================================================
# é…ç½®æ•°æ®ç»“æ„
# ============================================================

@dataclass
class LLMProfile:
    """å•ä¸ªLLMé…ç½®"""
    name: str                    # é…ç½®åç§°
    provider: str                # æä¾›å•†ç±»å‹
    api_base: str                # APIåœ°å€
    api_key_env: str = ""        # API Keyç¯å¢ƒå˜é‡åï¼ˆä¸ç›´æ¥å­˜å‚¨keyï¼‰
    api_key: str = ""            # ç›´æ¥é…ç½®çš„API Keyï¼ˆå¯é€‰ï¼Œç”¨äºconfig.jsonï¼‰
    model: str = ""              # é»˜è®¤æ¨¡å‹
    description: str = ""        # æè¿°
    models_available: List[str] = field(default_factory=list)  # å¯ç”¨æ¨¡å‹åˆ—è¡¨
    max_tokens: int = 2000
    temperature: float = 0.7
    # åœºæ™¯åŒ–æ¨¡å‹é…ç½®ï¼šä¸ºä¸åŒä»»åŠ¡åœºæ™¯æŒ‡å®šä¸“ç”¨æ¨¡å‹
    scenario_models: Dict[str, str] = field(default_factory=dict)

    def get_api_key(self) -> Optional[str]:
        """è·å–API Key - ä¼˜å…ˆä½¿ç”¨ç›´æ¥é…ç½®çš„keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–"""
        # ä¼˜å…ˆè¿”å›ç›´æ¥é…ç½®çš„api_keyï¼ˆæ¥è‡ªconfig.jsonï¼‰
        if self.api_key:
            return self.api_key
        # å¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        if self.api_key_env:
            return os.getenv(self.api_key_env)
        return None

    def is_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²é…ç½®API Key"""
        if self.provider == "ollama":
            return True  # Ollamaä¸éœ€è¦key
        return bool(self.get_api_key())

    def get_model_for_scenario(self, scenario: str) -> str:
        """
        è·å–æŒ‡å®šåœºæ™¯çš„æ¨¡å‹

        å¦‚æœåœºæ™¯æ²¡æœ‰é…ç½®ä¸“ç”¨æ¨¡å‹ï¼Œåˆ™è¿”å›é»˜è®¤æ¨¡å‹ã€‚

        Args:
            scenario: åœºæ™¯åç§°ï¼Œå¦‚ "tag_classification", "strategy_scan"

        Returns:
            è¯¥åœºæ™¯ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        return self.scenario_models.get(scenario, self.model)

    def is_reasoning_model(self, model: Optional[str] = None) -> bool:
        """
        åˆ¤æ–­æŒ‡å®šæ¨¡å‹æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹

        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹

        Returns:
            æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹
        """
        from llm_providers import is_reasoning_model
        model_name = model or self.model
        return is_reasoning_model(model_name)

    def get_reasoning_model(self) -> Optional[str]:
        """
        è·å–å¯ç”¨çš„æ€è€ƒæ¨¡å‹

        è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ€è€ƒæ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›Noneã€‚

        Returns:
            æ€è€ƒæ¨¡å‹åç§°æˆ–None
        """
        from llm_providers import is_reasoning_model
        for m in self.models_available:
            if is_reasoning_model(m):
                return m
        # æ£€æŸ¥é»˜è®¤æ¨¡å‹
        if is_reasoning_model(self.model):
            return self.model
        return None

    def get_fast_model(self) -> Optional[str]:
        """
        è·å–å¯ç”¨çš„å¿«é€Ÿæ¨¡å‹

        è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„éæ€è€ƒæ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›Noneã€‚

        Returns:
            å¿«é€Ÿæ¨¡å‹åç§°æˆ–None
        """
        from llm_providers import is_reasoning_model
        for m in self.models_available:
            if not is_reasoning_model(m):
                return m
        # æ£€æŸ¥é»˜è®¤æ¨¡å‹
        if not is_reasoning_model(self.model):
            return self.model
        return None


# ============================================================
# é¢„è®¾é…ç½®
# ============================================================

BUILTIN_PROFILES: Dict[str, LLMProfile] = {
    # SiliconFlow - å›½å†…èšåˆå¹³å°
    "siliconflow": LLMProfile(
        name="siliconflow",
        provider="openai_compatible",
        api_base="https://api.siliconflow.cn/v1",
        api_key_env="SILICONFLOW_API_KEY",
        model="deepseek-ai/DeepSeek-V3",
        description="SiliconFlow - å›½å†…èšåˆå¹³å°ï¼Œé€Ÿåº¦å¿«",
        models_available=[
            "Qwen/Qwen2.5-72B-Instruct",
            "Qwen/Qwen2.5-32B-Instruct",
            "Qwen/Qwen2.5-7B-Instruct",
            "deepseek-ai/DeepSeek-V3",
            "deepseek-ai/DeepSeek-R1",
            "Pro/deepseek-ai/DeepSeek-R1",
            "THUDM/glm-4-9b-chat",
            "Pro/zai-org/GLM-4.7",
        ],
        scenario_models={
            LLMScenario.TAG_CLASSIFICATION: "deepseek-ai/DeepSeek-R1",  # Tagåˆ†ç±»ç”¨æ€è€ƒæ¨¡å‹
            LLMScenario.STRATEGY_SCAN: "deepseek-ai/DeepSeek-V3",       # ç­–ç•¥æ‰«æç”¨å¿«é€Ÿæ¨¡å‹
        }
    ),

    # DeepSeekå®˜æ–¹
    "deepseek": LLMProfile(
        name="deepseek",
        provider="openai_compatible",
        api_base="https://api.deepseek.com/v1",
        api_key_env="DEEPSEEK_API_KEY",
        model="deepseek-chat",
        description="DeepSeekå®˜æ–¹ - ä¾¿å®œå¥½ç”¨",
        models_available=[
            "deepseek-chat",
            "deepseek-reasoner",
        ],
        scenario_models={
            LLMScenario.TAG_CLASSIFICATION: "deepseek-reasoner",    # Tagåˆ†ç±»ç”¨æ€è€ƒæ¨¡å‹
            LLMScenario.STRATEGY_SCAN: "deepseek-chat",            # ç­–ç•¥æ‰«æç”¨å¿«é€Ÿæ¨¡å‹
        }
    ),
    
    # OpenAI
    "openai": LLMProfile(
        name="openai",
        provider="openai",
        api_base="https://api.openai.com/v1",
        api_key_env="OPENAI_API_KEY",
        model="gpt-4o",
        description="OpenAI - GPTç³»åˆ—",
        models_available=[
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4-turbo",
            "gpt-3.5-turbo",
        ]
    ),
    
    # Anthropic Claude
    "anthropic": LLMProfile(
        name="anthropic",
        provider="anthropic",
        api_base="https://api.anthropic.com",
        api_key_env="ANTHROPIC_API_KEY",
        model="claude-sonnet-4-20250514",
        description="Anthropic - Claudeç³»åˆ—",
        models_available=[
            "claude-sonnet-4-20250514",
            "claude-3-5-sonnet-20241022",
            "claude-3-haiku-20240307",
        ]
    ),
    
    # é˜¿é‡Œäº‘é€šä¹‰
    "aliyun": LLMProfile(
        name="aliyun",
        provider="openai_compatible",
        api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key_env="DASHSCOPE_API_KEY",
        model="qwen-plus",
        description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®",
        models_available=[
            "qwen-turbo",
            "qwen-plus", 
            "qwen-max",
        ]
    ),
    
    # æ™ºè°±GLM
    "zhipu": LLMProfile(
        name="zhipu",
        provider="openai_compatible",
        api_base="https://open.bigmodel.cn/api/paas/v4",
        api_key_env="ZHIPU_API_KEY",
        model="glm-4-plus",
        description="æ™ºè°±AI - GLMç³»åˆ—",
        models_available=[
            "glm-4-plus",
            "glm-4",
            "glm-4-flash",
        ]
    ),
    
    # æœ¬åœ°Ollama
    "ollama": LLMProfile(
        name="ollama",
        provider="ollama",
        api_base="http://localhost:11434",
        api_key_env="",
        model="qwen2.5:7b",
        description="æœ¬åœ°Ollama - å…è´¹ç¦»çº¿",
        models_available=[
            "llama3.1:8b",
            "llama3.1:70b",
            "qwen2.5:7b",
            "qwen2.5:14b",
            "qwen2.5:32b",
            "deepseek-r1:7b",
            "deepseek-r1:14b",
        ],
        scenario_models={
            LLMScenario.TAG_CLASSIFICATION: "deepseek-r1:7b",   # Tagåˆ†ç±»ç”¨æ€è€ƒæ¨¡å‹
            LLMScenario.STRATEGY_SCAN: "qwen2.5:7b",           # ç­–ç•¥æ‰«æç”¨å¿«é€Ÿæ¨¡å‹
        }
    ),
    
    # OpenRouter - å›½å¤–èšåˆ
    "openrouter": LLMProfile(
        name="openrouter",
        provider="openai_compatible",
        api_base="https://openrouter.ai/api/v1",
        api_key_env="OPENROUTER_API_KEY",
        model="anthropic/claude-3.5-sonnet",
        description="OpenRouter - å›½å¤–èšåˆå¹³å°",
        models_available=[
            "anthropic/claude-3.5-sonnet",
            "openai/gpt-4o",
            "google/gemini-pro",
            "meta-llama/llama-3.1-70b-instruct",
        ]
    ),

    # ModelScope - é˜¿é‡Œäº‘æ¨¡å‹æ‰˜ç®¡å¹³å°
    "modelscope": LLMProfile(
        name="modelscope",
        provider="modelscope",
        api_base="https://api-inference.modelscope.cn/v1",
        api_key_env="MODELSCOPE_API_KEY",
        model="Qwen/Qwen2.5-72B-Instruct",
        description="ModelScope - é˜¿é‡Œäº‘æ¨¡å‹æ‰˜ç®¡å¹³å°",
        models_available=[
            "Qwen/Qwen2.5-72B-Instruct",
            "Qwen/Qwen2.5-32B-Instruct",
            "Qwen/Qwen2.5-14B-Instruct",
            "Qwen/Qwen2.5-7B-Instruct",
            "deepseek-ai/DeepSeek-V3",
            "deepseek-ai/DeepSeek-R1",
        ],
        scenario_models={
            LLMScenario.TAG_CLASSIFICATION: "deepseek-ai/DeepSeek-R1",  # Tagåˆ†ç±»ç”¨æ€è€ƒæ¨¡å‹
            LLMScenario.STRATEGY_SCAN: "Qwen/Qwen2.5-32B-Instruct",     # ç­–ç•¥æ‰«æç”¨å¿«é€Ÿæ¨¡å‹
        }
    ),
}


# ============================================================
# é…ç½®ç®¡ç†å™¨
# ============================================================

class LLMConfigManager:
    """LLMé…ç½®ç®¡ç†å™¨"""
    
    CONFIG_FILE = "config.json"

    def __init__(self):
        self.profiles: Dict[str, LLMProfile] = BUILTIN_PROFILES.copy()
        self._load_custom_profiles()

    def _load_custom_profiles(self):
        """åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰é…ç½® - ä»config.jsonçš„llm_profilesåŒºæ®µè¯»å–"""
        config_path = Path(self.CONFIG_FILE)
        if config_path.exists():
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # ä»config.jsonçš„llm_profilesåŒºæ®µè¯»å–
                llm_profiles = data.get("llm_profiles", {})

                for name, profile_data in llm_profiles.items():
                    # è¿‡æ»¤æ‰ä»¥_å¼€å¤´çš„æ³¨é‡Šå­—æ®µå’Œä¸æ”¯æŒçš„å­—æ®µ
                    unsupported_fields = {'embedding_model'}
                    filtered_data = {
                        k: v for k, v in profile_data.items()
                        if not k.startswith('_') and k not in unsupported_fields
                    }

                    # åˆ›å»ºLLMProfileï¼Œç¡®ä¿nameå­—æ®µå­˜åœ¨
                    if "name" not in filtered_data:
                        filtered_data["name"] = name

                    # å¦‚æœæ²¡æœ‰api_key_envï¼Œè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²ï¼ˆå…è®¸ä½¿ç”¨api_keyï¼‰
                    if "api_key_env" not in filtered_data:
                        filtered_data["api_key_env"] = ""

                    # è¿‡æ»¤scenario_modelsä¸­çš„æ³¨é‡Šå­—æ®µ
                    if "scenario_models" in filtered_data and isinstance(filtered_data["scenario_models"], dict):
                        filtered_data["scenario_models"] = {
                            k: v for k, v in filtered_data["scenario_models"].items()
                            if not k.startswith('_')
                        }

                    self.profiles[name] = LLMProfile(**filtered_data)
            except Exception as e:
                # ä½¿ç”¨ç®€å•çš„ASCIIå­—ç¬¦é¿å…ç¼–ç é—®é¢˜
                print(f"[WARNING] Failed to load custom profiles: {e}")
    
    def save_custom_profile(self, profile: LLMProfile):
        """ä¿å­˜è‡ªå®šä¹‰é…ç½®"""
        config_path = Path(self.CONFIG_FILE)
        
        # è¯»å–ç°æœ‰é…ç½®
        data = {"profiles": {}}
        if config_path.exists():
            with open(config_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        # æ·»åŠ /æ›´æ–°é…ç½®
        data["profiles"][profile.name] = asdict(profile)
        
        # ä¿å­˜
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        self.profiles[profile.name] = profile
    
    def get_profile(self, name: str) -> Optional[LLMProfile]:
        """è·å–é…ç½®"""
        return self.profiles.get(name)
    
    def list_profiles(self) -> List[LLMProfile]:
        """åˆ—å‡ºæ‰€æœ‰é…ç½®"""
        return list(self.profiles.values())
    
    def get_configured_profiles(self) -> List[LLMProfile]:
        """è·å–å·²é…ç½®API Keyçš„é…ç½®"""
        return [p for p in self.profiles.values() if p.is_configured()]
    
    def detect_profile(self) -> Optional[LLMProfile]:
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„é…ç½®"""
        # ä¼˜å…ˆçº§é¡ºåº
        priority = ["siliconflow", "deepseek", "modelscope", "aliyun", "zhipu", "openai", "anthropic", "ollama"]

        for name in priority:
            profile = self.profiles.get(name)
            if profile and profile.is_configured():
                return profile

        return None
    
    def get_active_profile(self) -> Optional[LLMProfile]:
        """
        è·å–å½“å‰æ¿€æ´»çš„é…ç½®
        
        ä¼˜å…ˆçº§ï¼š
        1. ç¯å¢ƒå˜é‡ LLM_PROFILE
        2. è‡ªåŠ¨æ£€æµ‹
        """
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        profile_name = os.getenv("LLM_PROFILE")
        if profile_name:
            profile = self.get_profile(profile_name)
            if profile:
                return profile
            print(f"âš ï¸ æœªæ‰¾åˆ°é…ç½®: {profile_name}")
        
        # è‡ªåŠ¨æ£€æµ‹
        return self.detect_profile()


# ============================================================
# å‘½ä»¤è¡Œå·¥å…·
# ============================================================

def print_profiles_table(profiles: List[LLMProfile], show_status: bool = True):
    """æ‰“å°é…ç½®è¡¨æ ¼

    Args:
        profiles: è¦æ˜¾ç¤ºçš„é…ç½®åˆ—è¡¨
        show_status: æ˜¯å¦æ˜¾ç¤ºé…ç½®çŠ¶æ€
    """
    print("\n" + "=" * 80)
    print("å¯ç”¨çš„LLMé…ç½®")
    print("=" * 80)

    for p in profiles:
        if p.is_configured():
            status_icon = "[OK]"
            status_text = "å·²é…ç½®"
        else:
            status_icon = "[--]"
            status_text = f"æœªé…ç½® (éœ€è¦è®¾ç½® {p.api_key_env})"

        if not show_status:
            status_icon = "    "

        print(f"\n{status_icon} [{p.name}]")
        print(f"   æè¿°: {p.description}")
        print(f"   é»˜è®¤æ¨¡å‹: {p.model}")
        if show_status:
            print(f"   çŠ¶æ€: {status_text}")
        if p.models_available:
            models_str = ', '.join(p.models_available[:4])
            if len(p.models_available) > 4:
                models_str += '...'
            print(f"   å¯ç”¨æ¨¡å‹: {models_str}")

    # æ±‡æ€»ç»Ÿè®¡
    configured = [p for p in profiles if p.is_configured()]
    print("\n" + "-" * 80)
    print(f"æ±‡æ€»: {len(configured)}/{len(profiles)} ä¸ªé…ç½®å·²å°±ç»ª")
    if configured:
        ready_names = ', '.join(p.name for p in configured)
        print(f"å¯ç”¨é…ç½®: {ready_names}")
    else:
        print("âš ï¸  æ²¡æœ‰å·²é…ç½®çš„profileï¼Œè¯·è®¾ç½®å¯¹åº”çš„API Keyç¯å¢ƒå˜é‡")


def test_profile(profile: LLMProfile) -> bool:
    """æµ‹è¯•é…ç½®æ˜¯å¦å¯ç”¨"""
    print(f"\næµ‹è¯•é…ç½®: {profile.name}")
    print(f"  API Base: {profile.api_base}")
    print(f"  Model: {profile.model}")
    
    if not profile.is_configured():
        print(f"  âŒ æœªé…ç½® API Key (éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡: {profile.api_key_env})")
        return False
    
    try:
        # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        from llm_providers import create_llm_client
        
        client = create_llm_client(
            provider=profile.provider,
            api_base=profile.api_base,
            api_key=profile.get_api_key(),
            model=profile.model,
        )
        
        print("  å‘é€æµ‹è¯•è¯·æ±‚...")
        response = client.chat("è¯´'æµ‹è¯•æˆåŠŸ'è¿™ä¸‰ä¸ªå­—")
        print(f"  âœ… å“åº”: {response.content[:50]}...")
        client.close()
        return True
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def interactive_add_profile():
    """äº¤äº’å¼æ·»åŠ é…ç½®"""
    print("\næ·»åŠ æ–°çš„LLMé…ç½®")
    print("-" * 40)
    
    name = input("é…ç½®åç§° (å¦‚ my-llm): ").strip()
    if not name:
        print("å–æ¶ˆ")
        return
    
    provider = input("æä¾›å•†ç±»å‹ [openai_compatible]: ").strip() or "openai_compatible"
    api_base = input("APIåœ°å€: ").strip()
    api_key_env = input("API Keyç¯å¢ƒå˜é‡å: ").strip()
    model = input("é»˜è®¤æ¨¡å‹: ").strip()
    description = input("æè¿° (å¯é€‰): ").strip()
    
    profile = LLMProfile(
        name=name,
        provider=provider,
        api_base=api_base,
        api_key_env=api_key_env,
        model=model,
        description=description,
    )
    
    manager = LLMConfigManager()
    manager.save_custom_profile(profile)
    print(f"\nâœ… å·²ä¿å­˜é…ç½®: {name}")


def generate_env_template():
    """ç”Ÿæˆç¯å¢ƒå˜é‡æ¨¡æ¿"""
    template = """# LLM API Keys é…ç½®æ¨¡æ¿
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env å¹¶å¡«å…¥ä½ çš„API Key

# é€‰æ‹©æ¿€æ´»çš„é…ç½® (å¯é€‰ï¼Œä¸è®¾ç½®åˆ™è‡ªåŠ¨æ£€æµ‹)
# LLM_PROFILE=siliconflow

# SiliconFlow (å›½å†…èšåˆå¹³å°ï¼Œæ¨è)
SILICONFLOW_API_KEY=sk-your-key-here

# DeepSeek (å®˜æ–¹ï¼Œä¾¿å®œ)
DEEPSEEK_API_KEY=sk-your-key-here

# OpenAI
OPENAI_API_KEY=sk-your-key-here

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-your-key-here

# é˜¿é‡Œäº‘é€šä¹‰åƒé—®
DASHSCOPE_API_KEY=sk-your-key-here

# æ™ºè°±GLM
ZHIPU_API_KEY=your-key-here

# OpenRouter (å›½å¤–èšåˆ)
OPENROUTER_API_KEY=sk-or-your-key-here
"""
    
    with open(".env.template", "w", encoding="utf-8") as f:
        f.write(template)
    print("âœ… å·²ç”Ÿæˆ .env.template")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLMé…ç½®ç®¡ç†å·¥å…·")
    parser.add_argument("--list", "-l", action="store_true", help="åˆ—å‡ºæ‰€æœ‰é…ç½®")
    parser.add_argument("--test", "-t", type=str, help="æµ‹è¯•æŒ‡å®šé…ç½®")
    parser.add_argument("--add", "-a", action="store_true", help="æ·»åŠ æ–°é…ç½®")
    parser.add_argument("--env-template", action="store_true", help="ç”Ÿæˆç¯å¢ƒå˜é‡æ¨¡æ¿")
    parser.add_argument("--detect", "-d", action="store_true", help="è‡ªåŠ¨æ£€æµ‹å¯ç”¨é…ç½®")
    
    args = parser.parse_args()
    
    manager = LLMConfigManager()
    
    if args.list:
        print_profiles_table(manager.list_profiles())
        
    elif args.test:
        profile = manager.get_profile(args.test)
        if profile:
            test_profile(profile)
        else:
            print(f"âŒ æœªæ‰¾åˆ°é…ç½®: {args.test}")
            
    elif args.add:
        interactive_add_profile()
        
    elif args.env_template:
        generate_env_template()
        
    elif args.detect:
        profile = manager.detect_profile()
        if profile:
            print(f"âœ… æ£€æµ‹åˆ°å¯ç”¨é…ç½®: {profile.name}")
            print(f"   æ¨¡å‹: {profile.model}")
        else:
            print("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨é…ç½®ï¼Œè¯·è®¾ç½®API Key")
            print("\næç¤ºï¼šè¿è¡Œ python llm_config.py --list æŸ¥çœ‹æ‰€æœ‰é…ç½®")
            
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©å’ŒçŠ¶æ€
        print("\nLLMé…ç½®ç®¡ç†å·¥å…·")
        print("=" * 50)
        
        configured = manager.get_configured_profiles()
        if configured:
            print(f"\nâœ… å·²é…ç½®çš„æä¾›å•†: {', '.join(p.name for p in configured)}")
            
            active = manager.get_active_profile()
            if active:
                print(f"ğŸ“Œ å½“å‰æ¿€æ´»: {active.name} ({active.model})")
        else:
            print("\nâŒ æœªæ£€æµ‹åˆ°ä»»ä½•å·²é…ç½®çš„API Key")
        
        print("\nå¸¸ç”¨å‘½ä»¤:")
        print("  python llm_config.py --list        # æŸ¥çœ‹æ‰€æœ‰é…ç½®")
        print("  python llm_config.py --test NAME   # æµ‹è¯•é…ç½®")
        print("  python llm_config.py --env-template # ç”Ÿæˆ.envæ¨¡æ¿")
        print("  python llm_config.py --add         # æ·»åŠ è‡ªå®šä¹‰é…ç½®")


# ============================================================
# ä¾¿æ·å‡½æ•°ï¼ˆä¾›å…¶ä»–æ¨¡å—è°ƒç”¨ï¼‰
# ============================================================

def get_llm_config() -> Optional[LLMProfile]:
    """è·å–å½“å‰LLMé…ç½®ï¼ˆä¾›æ‰«æå™¨è°ƒç”¨ï¼‰"""
    manager = LLMConfigManager()
    return manager.get_active_profile()


def get_llm_config_by_name(name: str) -> Optional[LLMProfile]:
    """æ ¹æ®åç§°è·å–é…ç½®"""
    manager = LLMConfigManager()
    return manager.get_profile(name)


def list_available_profiles() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®å"""
    manager = LLMConfigManager()
    return [p.name for p in manager.list_profiles()]


if __name__ == "__main__":
    main()
