#!/usr/bin/env python3
"""
ç³»ç»Ÿé…ç½®æ–‡ä»¶
============

æ”¯æŒä»ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶åŠ è½½é…ç½®
"""

import os
import json
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any, List


@dataclass
class LLMSettings:
    """LLMé…ç½®"""
    # æä¾›å•†: openai / anthropic / aliyun / zhipu / deepseek / ollama / openai_compatible
    provider: str = "openai"
    
    # æ¨¡å‹åç§°ï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤ï¼‰
    model: str = ""
    
    # APIå¯†é’¥ï¼ˆç•™ç©ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    api_key: str = ""
    
    # APIåœ°å€ï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤ï¼‰
    api_base: str = ""
    
    # ç”Ÿæˆå‚æ•°
    max_tokens: int = 2000
    temperature: float = 0.7
    timeout: int = 60


@dataclass
class ScanSettings:
    """æ‰«æé…ç½®"""
    # è·å–å¸‚åœºæ•°é‡
    market_limit: int = 200

    # ç›¸ä¼¼åº¦é˜ˆå€¼
    similarity_threshold: float = 0.3

    # æœ€å°åˆ©æ¶¦ç™¾åˆ†æ¯”
    min_profit_pct: float = 2.0

    # æœ€å°æµåŠ¨æ€§ï¼ˆUSDCï¼‰
    min_liquidity: float = 10000

    # æœ€å°LLMç½®ä¿¡åº¦
    min_confidence: float = 0.8

    # æ¯æ¬¡æ‰«ææœ€å¤§LLMè°ƒç”¨æ¬¡æ•°
    max_llm_calls: int = 300

    # ğŸ†• å‘é‡åŒ–ç›¸å…³é…ç½®
    # æ˜¯å¦å¯ç”¨è¯­ä¹‰èšç±»ï¼ˆå‘é‡åŒ–æ¨¡å¼ï¼‰
    use_semantic_clustering: bool = True

    # èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
    semantic_threshold: float = 0.85

    # Embeddingæ¨¡å‹åç§°
    embedding_model: str = "BAAI/bge-large-zh-v1.5"

    # ğŸ†• ç¼“å­˜ç›¸å…³é…ç½®
    # æ˜¯å¦å¯ç”¨å¸‚åœºæ•°æ®ç¼“å­˜
    enable_cache: bool = True

    # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
    cache_ttl: int = 3600

    # ğŸ†• é¢†åŸŸç›¸å…³é…ç½®
    # æ‰«æçš„é»˜è®¤å¸‚åœºé¢†åŸŸ
    scan_domain: str = "crypto"

    # ğŸ†• å­ç±»åˆ«ç­›é€‰é…ç½®ï¼ˆv2.1æ–°å¢ï¼‰
    # å­ç±»åˆ«ç­›é€‰åˆ—è¡¨ï¼Œç•™ç©ºè¡¨ç¤ºè·å–æ•´ä¸ªé¢†åŸŸçš„å¸‚åœº
    # æ”¯æŒç®€å†™ï¼Œå¦‚ ["btc", "eth"] ç­‰åŒäº ["bitcoin", "ethereum"]
    # ä¼šè‡ªåŠ¨åŒ…å«ç›¸å…³æ ‡ç­¾ï¼Œå¦‚ "bitcoin" è‡ªåŠ¨åŒ…å« "bitcoin-prices", "bitcoin-volatility" ç­‰
    scan_subcategories: List[str] = field(default_factory=list)

    # ğŸ†• åˆ†é¡µç›¸å…³é…ç½®
    # æ˜¯å¦å¯ç”¨å…¨é‡è·å–ï¼ˆé»˜è®¤False=ä¿æŒæ—§è¡Œä¸ºï¼Œæœ€å¤š100ä¸ªå¸‚åœºï¼‰
    enable_full_fetch: bool = False

    # æ¯é¡µå¤§å°
    fetch_page_size: int = 100

    # æ¯ä¸ªtagæœ€å¤§è·å–æ•°é‡ï¼ˆ0=å…¨é‡è·å–ï¼‰
    fetch_max_per_tag: int = 0

    # APIè¯·æ±‚é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰
    fetch_rate_limit: float = 2.0


@dataclass
class OutputSettings:
    """è¾“å‡ºé…ç½®"""
    # è¾“å‡ºç›®å½•
    output_dir: str = "./output"
    
    # ç¼“å­˜ç›®å½•
    cache_dir: str = "./cache"
    
    # æ—¥å¿—çº§åˆ«: DEBUG / INFO / WARNING / ERROR
    log_level: str = "INFO"
    
    # è¯¦ç»†æ—¥å¿—
    detailed_log: bool = True


@dataclass
class Config:
    """ä¸»é…ç½®ç±»"""
    llm: LLMSettings = field(default_factory=LLMSettings)
    scan: ScanSettings = field(default_factory=ScanSettings)
    output: OutputSettings = field(default_factory=OutputSettings)

    # ğŸ†• å¤šLLMé…ç½®æ”¯æŒ
    llm_profiles: Dict[str, Any] = field(default_factory=dict)  # å¤šä¸ªLLMé…ç½®
    active_profile: str = ""  # å½“å‰æ¿€æ´»çš„profileåç§°

    @classmethod
    def from_env(cls) -> "Config":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            llm=LLMSettings(
                provider=os.getenv("LLM_PROVIDER", "openai"),
                model=os.getenv("LLM_MODEL", ""),
                api_key=os.getenv("LLM_API_KEY", ""),
                api_base=os.getenv("LLM_API_BASE", ""),
                max_tokens=int(os.getenv("LLM_MAX_TOKENS", "2000")),
                temperature=float(os.getenv("LLM_TEMPERATURE", "0.7")),
            ),
            scan=ScanSettings(
                market_limit=int(os.getenv("MARKET_LIMIT", "200")),
                min_profit_pct=float(os.getenv("MIN_PROFIT_PCT", "2.0")),
                min_liquidity=float(os.getenv("MIN_LIQUIDITY", "10000")),
                min_confidence=float(os.getenv("MIN_CONFIDENCE", "0.8")),
                use_semantic_clustering=os.getenv("USE_SEMANTIC_CLUSTERING", "true").lower() == "true",
                semantic_threshold=float(os.getenv("SEMANTIC_THRESHOLD", "0.85")),
                enable_cache=os.getenv("ENABLE_CACHE", "true").lower() == "true",
                cache_ttl=int(os.getenv("CACHE_TTL", "3600")),
                scan_domain=os.getenv("SCAN_DOMAIN", "crypto"),
                scan_subcategories=os.getenv("SCAN_SUBCATEGORIES", "").split(",") if os.getenv("SCAN_SUBCATEGORIES") else [],
            ),
            output=OutputSettings(
                output_dir=os.getenv("OUTPUT_DIR", "./output"),
                log_level=os.getenv("LOG_LEVEL", "INFO"),
                detailed_log=os.getenv("DETAILED_LOG", "true").lower() == "true",
            )
        )
    
    @classmethod
    def from_file(cls, path: str) -> "Config":
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        def _filter_comments(d: dict) -> dict:
            """è¿‡æ»¤æ‰ä»¥ _ å¼€å¤´çš„æ³¨é‡Šå­—æ®µ"""
            return {k: v for k, v in d.items() if not k.startswith('_')}

        def _filter_for_llm_settings(d: dict) -> dict:
            """è¿‡æ»¤æ‰ LLMSettings ä¸æ”¯æŒçš„å­—æ®µï¼ˆå¦‚ embedding_modelï¼‰"""
            allowed = {'provider', 'model', 'api_key', 'api_base', 'max_tokens', 'temperature', 'timeout'}
            return {k: v for k, v in d.items() if k in allowed}

        # è·å– llm_profiles å’Œ active_profile
        llm_profiles = data.get("llm_profiles", {})
        active_profile = data.get("active_profile", "")

        # ä» active_profile åŠ è½½ llm é…ç½®ï¼ˆä¸å†æ”¯æŒç‹¬ç«‹çš„ llm å­—æ®µï¼‰
        llm_data = {}
        if active_profile and active_profile in llm_profiles:
            llm_data = llm_profiles[active_profile]

        return cls(
            llm=LLMSettings(**_filter_for_llm_settings(llm_data)) if llm_data else LLMSettings(),
            scan=ScanSettings(**_filter_comments(data.get("scan", {}))),
            output=OutputSettings(**_filter_comments(data.get("output", {}))),
            llm_profiles=llm_profiles,
            active_profile=active_profile,
        )
    
    def to_file(self, path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        data = {
            "llm": asdict(self.llm),
            "scan": asdict(self.scan),
            "output": asdict(self.output),
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def get_llm_profile(self, profile_name: Optional[str] = None) -> LLMSettings:
        """
        è·å–æŒ‡å®šprofileçš„LLMé…ç½®

        ä¼˜å…ˆçº§ï¼š
        1. æŒ‡å®šçš„profile_nameï¼ˆå¦‚æœå­˜åœ¨äºllm_profilesä¸­ï¼‰
        2. active_profileæŒ‡å®šçš„é…ç½®
        3. llmå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰

        Args:
            profile_name: profileåç§°ï¼Œç•™ç©ºåˆ™ä½¿ç”¨active_profile

        Returns:
            LLMSettingsé…ç½®å¯¹è±¡
        """
        name = profile_name or self.active_profile

        if name and name in self.llm_profiles:
            profile_data = self.llm_profiles[name]
            return LLMSettings(
                provider=profile_data.get("provider", "openai"),
                model=profile_data.get("model", ""),
                api_key=profile_data.get("api_key", ""),
                api_base=profile_data.get("api_base", ""),
                max_tokens=profile_data.get("max_tokens", 2000),
                temperature=profile_data.get("temperature", 0.7),
                timeout=profile_data.get("timeout", 60),
            )

        # å›é€€åˆ°llmå­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
        return self.llm

    @classmethod
    def load(cls, config_path: Optional[str] = None) -> "Config":
        """
        åŠ è½½é…ç½®
        
        ä¼˜å…ˆçº§ï¼š
        1. æŒ‡å®šçš„é…ç½®æ–‡ä»¶
        2. å½“å‰ç›®å½•çš„ config.json
        3. ç¯å¢ƒå˜é‡
        4. é»˜è®¤å€¼
        """
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        if config_path and os.path.exists(config_path):
            return cls.from_file(config_path)
        
        if os.path.exists("config.json"):
            return cls.from_file("config.json")
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        return cls.from_env()


# é»˜è®¤é…ç½®æ¨¡æ¿
DEFAULT_CONFIG_TEMPLATE = """{
  "llm": {
    "provider": "openai",
    "model": "",
    "api_key": "",
    "api_base": "",
    "max_tokens": 2000,
    "temperature": 0.7,
    "timeout": 60
  },
  "scan": {
    "market_limit": 200,
    "similarity_threshold": 0.3,
    "min_profit_pct": 2.0,
    "min_liquidity": 10000,
    "min_confidence": 0.8,
    "max_llm_calls": 30,
    "use_semantic_clustering": true,
    "semantic_threshold": 0.85,
    "embedding_model": "BAAI/bge-large-zh-v1.5",
    "enable_cache": true,
    "cache_ttl": 3600,
    "scan_domain": "crypto"
  },
  "output": {
    "output_dir": "./output",
    "cache_dir": "./cache",
    "log_level": "INFO",
    "detailed_log": true
  }
}
"""


def create_default_config(path: str = "config.json"):
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    with open(path, "w", encoding="utf-8") as f:
        f.write(DEFAULT_CONFIG_TEMPLATE)
    print(f"âœ… å·²åˆ›å»ºé…ç½®æ–‡ä»¶: {path}")


