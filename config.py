#!/usr/bin/env python3
"""
ç³»ç»Ÿé…ç½®æ–‡ä»¶
============

æ”¯æŒä»ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶åŠ è½½é…ç½®
"""

import os
import json
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any


@dataclass
class LLMSettings:
    """LLMé…ç½®"""
    # æä¾›å•†: openai / anthropic / aliyun / zhipu / deepseek / ollama / openai_compatible
    provider: str = "openai"
    
    # æ¨¡å‹åç§°ï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤ï¼‰
    model: str = ""
    
    # APIå¯†é’¥ï¼ˆç•™ç©ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    api_key: str = ""
    
    # APIåœ°å€ï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤ï¼‰
    api_base: str = ""
    
    # ç”Ÿæˆå‚æ•°
    max_tokens: int = 2000
    temperature: float = 0.7
    timeout: int = 60


@dataclass
class ScanSettings:
    """æ‰«æé…ç½®"""
    # è·å–å¸‚åœºæ•°é‡
    market_limit: int = 200

    # ç›¸ä¼¼åº¦é˜ˆå€¼
    similarity_threshold: float = 0.3

    # æœ€å°åˆ©æ¶¦ç™¾åˆ†æ¯”
    min_profit_pct: float = 2.0

    # æœ€å°æµåŠ¨æ€§ï¼ˆUSDCï¼‰
    min_liquidity: float = 10000

    # æœ€å°LLMç½®ä¿¡åº¦
    min_confidence: float = 0.8

    # æ¯æ¬¡æ‰«ææœ€å¤§LLMè°ƒç”¨æ¬¡æ•°
    max_llm_calls: int = 30

    # ğŸ†• å‘é‡åŒ–ç›¸å…³é…ç½®
    # æ˜¯å¦å¯ç”¨è¯­ä¹‰èšç±»ï¼ˆå‘é‡åŒ–æ¨¡å¼ï¼‰
    use_semantic_clustering: bool = True

    # èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
    semantic_threshold: float = 0.85

    # Embeddingæ¨¡å‹åç§°
    embedding_model: str = "BAAI/bge-large-zh-v1.5"

    # ğŸ†• ç¼“å­˜ç›¸å…³é…ç½®
    # æ˜¯å¦å¯ç”¨å¸‚åœºæ•°æ®ç¼“å­˜
    enable_cache: bool = True

    # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
    cache_ttl: int = 3600

    # ğŸ†• é¢†åŸŸç›¸å…³é…ç½®
    # æ‰«æçš„é»˜è®¤å¸‚åœºé¢†åŸŸ
    scan_domain: str = "crypto"


@dataclass
class OutputSettings:
    """è¾“å‡ºé…ç½®"""
    # è¾“å‡ºç›®å½•
    output_dir: str = "./output"
    
    # ç¼“å­˜ç›®å½•
    cache_dir: str = "./cache"
    
    # æ—¥å¿—çº§åˆ«: DEBUG / INFO / WARNING / ERROR
    log_level: str = "INFO"
    
    # è¯¦ç»†æ—¥å¿—
    detailed_log: bool = True


@dataclass
class Config:
    """ä¸»é…ç½®ç±»"""
    llm: LLMSettings = field(default_factory=LLMSettings)
    scan: ScanSettings = field(default_factory=ScanSettings)
    output: OutputSettings = field(default_factory=OutputSettings)
    
    @classmethod
    def from_env(cls) -> "Config":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            llm=LLMSettings(
                provider=os.getenv("LLM_PROVIDER", "openai"),
                model=os.getenv("LLM_MODEL", ""),
                api_key=os.getenv("LLM_API_KEY", ""),
                api_base=os.getenv("LLM_API_BASE", ""),
                max_tokens=int(os.getenv("LLM_MAX_TOKENS", "2000")),
                temperature=float(os.getenv("LLM_TEMPERATURE", "0.7")),
            ),
            scan=ScanSettings(
                market_limit=int(os.getenv("MARKET_LIMIT", "200")),
                min_profit_pct=float(os.getenv("MIN_PROFIT_PCT", "2.0")),
                min_liquidity=float(os.getenv("MIN_LIQUIDITY", "10000")),
                min_confidence=float(os.getenv("MIN_CONFIDENCE", "0.8")),
                use_semantic_clustering=os.getenv("USE_SEMANTIC_CLUSTERING", "true").lower() == "true",
                semantic_threshold=float(os.getenv("SEMANTIC_THRESHOLD", "0.85")),
                enable_cache=os.getenv("ENABLE_CACHE", "true").lower() == "true",
                cache_ttl=int(os.getenv("CACHE_TTL", "3600")),
                scan_domain=os.getenv("SCAN_DOMAIN", "crypto"),
            ),
            output=OutputSettings(
                output_dir=os.getenv("OUTPUT_DIR", "./output"),
                log_level=os.getenv("LOG_LEVEL", "INFO"),
                detailed_log=os.getenv("DETAILED_LOG", "true").lower() == "true",
            )
        )
    
    @classmethod
    def from_file(cls, path: str) -> "Config":
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        return cls(
            llm=LLMSettings(**data.get("llm", {})),
            scan=ScanSettings(**data.get("scan", {})),
            output=OutputSettings(**data.get("output", {})),
        )
    
    def to_file(self, path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        data = {
            "llm": asdict(self.llm),
            "scan": asdict(self.scan),
            "output": asdict(self.output),
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    @classmethod
    def load(cls, config_path: Optional[str] = None) -> "Config":
        """
        åŠ è½½é…ç½®
        
        ä¼˜å…ˆçº§ï¼š
        1. æŒ‡å®šçš„é…ç½®æ–‡ä»¶
        2. å½“å‰ç›®å½•çš„ config.json
        3. ç¯å¢ƒå˜é‡
        4. é»˜è®¤å€¼
        """
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        if config_path and os.path.exists(config_path):
            return cls.from_file(config_path)
        
        if os.path.exists("config.json"):
            return cls.from_file("config.json")
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        return cls.from_env()


# é»˜è®¤é…ç½®æ¨¡æ¿
DEFAULT_CONFIG_TEMPLATE = """{
  "llm": {
    "provider": "openai",
    "model": "",
    "api_key": "",
    "api_base": "",
    "max_tokens": 2000,
    "temperature": 0.7,
    "timeout": 60
  },
  "scan": {
    "market_limit": 200,
    "similarity_threshold": 0.3,
    "min_profit_pct": 2.0,
    "min_liquidity": 10000,
    "min_confidence": 0.8,
    "max_llm_calls": 30,
    "use_semantic_clustering": true,
    "semantic_threshold": 0.85,
    "embedding_model": "BAAI/bge-large-zh-v1.5",
    "enable_cache": true,
    "cache_ttl": 3600,
    "scan_domain": "crypto"
  },
  "output": {
    "output_dir": "./output",
    "cache_dir": "./cache",
    "log_level": "INFO",
    "detailed_log": true
  }
}
"""


def create_default_config(path: str = "config.json"):
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    with open(path, "w", encoding="utf-8") as f:
        f.write(DEFAULT_CONFIG_TEMPLATE)
    print(f"âœ… å·²åˆ›å»ºé…ç½®æ–‡ä»¶: {path}")


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®åŠ è½½
    config = Config.load()
    print("å½“å‰é…ç½®:")
    print(f"  LLM Provider: {config.llm.provider}")
    print(f"  LLM Model: {config.llm.model or '(é»˜è®¤)'}")
    print(f"  Min Profit: {config.scan.min_profit_pct}%")
    print(f"  Log Level: {config.output.log_level}")
