#!/usr/bin/env python3
"""
LLMæä¾›å•†æŠ½è±¡å±‚
================

æ”¯æŒå¤šç§å¤§æ¨¡å‹APIçš„ç»Ÿä¸€æ¥å£ï¼ŒåŒ…æ‹¬ï¼š
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- é˜¿é‡Œäº‘ (é€šä¹‰åƒé—®)
- ç™¾åº¦ (æ–‡å¿ƒä¸€è¨€)
- æ™ºè°± (GLM-4)
- DeepSeek
- æœ¬åœ°æ¨¡å‹ (Ollama)
- OpenAIå…¼å®¹æ¥å£ (å¦‚vLLM, LocalAI, OneAPIç­‰)

ä½¿ç”¨æ–¹æ³•ï¼š
    from llm_providers import create_llm_client
    
    # é€šè¿‡é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
    client = create_llm_client(provider="openai", model="gpt-4o")
    
    # æˆ–è€…ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨æ£€æµ‹
    client = create_llm_client()
    
    # è°ƒç”¨
    response = client.chat("ä½ å¥½")
"""

import os
import json
import httpx
import logging
import traceback
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
from enum import Enum

# è·å–logger
logger = logging.getLogger(__name__)


# ============================================================
# é…ç½®å’Œå¸¸é‡
# ============================================================

class LLMProvider(Enum):
    """æ”¯æŒçš„LLMæä¾›å•†"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    ALIYUN = "aliyun"          # é€šä¹‰åƒé—®
    BAIDU = "baidu"            # æ–‡å¿ƒä¸€è¨€
    ZHIPU = "zhipu"            # æ™ºè°±GLM
    DEEPSEEK = "deepseek"
    OLLAMA = "ollama"          # æœ¬åœ°Ollama
    OPENAI_COMPATIBLE = "openai_compatible"  # OpenAIå…¼å®¹æ¥å£
    MODELSCOPE = "modelscope"  # ModelScope - é˜¿é‡Œäº‘æ¨¡å‹æ‰˜ç®¡å¹³å°


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.7
    timeout: int = 60
    
    # é¢å¤–å‚æ•°ï¼ˆä¸åŒæä¾›å•†å¯èƒ½éœ€è¦ï¼‰
    extra_params: Optional[Dict[str, Any]] = None


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    usage: Optional[Dict[str, int]] = None  # {"prompt_tokens": x, "completion_tokens": y}
    raw_response: Optional[Dict] = None
    reasoning_content: Optional[str] = None  # ğŸ†• æ€è€ƒæ¨¡å‹çš„æ¨ç†å†…å®¹


# ============================================================
# é»˜è®¤æ¨¡å‹é…ç½®
# ============================================================

DEFAULT_MODELS = {
    LLMProvider.OPENAI: "gpt-4o",
    LLMProvider.ANTHROPIC: "claude-sonnet-4-20250514",
    LLMProvider.ALIYUN: "qwen-plus",
    LLMProvider.BAIDU: "ernie-4.0-8k",
    LLMProvider.ZHIPU: "glm-4-plus",
    LLMProvider.DEEPSEEK: "deepseek-chat",
    LLMProvider.OLLAMA: "llama3.1:8b",
    LLMProvider.OPENAI_COMPATIBLE: "gpt-4o",
    LLMProvider.MODELSCOPE: "Qwen/Qwen2.5-72B-Instruct",
}

API_BASES = {
    LLMProvider.OPENAI: "https://api.openai.com/v1",
    LLMProvider.ANTHROPIC: "https://api.anthropic.com",
    LLMProvider.ALIYUN: "https://dashscope.aliyuncs.com/compatible-mode/v1",
    LLMProvider.BAIDU: "https://aip.baidubce.com",
    LLMProvider.ZHIPU: "https://open.bigmodel.cn/api/paas/v4",
    LLMProvider.DEEPSEEK: "https://api.deepseek.com/v1",
    LLMProvider.OLLAMA: "http://localhost:11434",
    LLMProvider.MODELSCOPE: "https://api-inference.modelscope.cn/v1",
}

# ç¯å¢ƒå˜é‡åæ˜ å°„
ENV_API_KEYS = {
    LLMProvider.OPENAI: "OPENAI_API_KEY",
    LLMProvider.ANTHROPIC: "ANTHROPIC_API_KEY",
    LLMProvider.ALIYUN: "DASHSCOPE_API_KEY",
    LLMProvider.BAIDU: "QIANFAN_API_KEY",
    LLMProvider.ZHIPU: "ZHIPU_API_KEY",
    LLMProvider.DEEPSEEK: "DEEPSEEK_API_KEY",
    LLMProvider.OPENAI_COMPATIBLE: "LLM_API_KEY",
    LLMProvider.MODELSCOPE: "MODELSCOPE_API_KEY",
}


# ============================================================
# æŠ½è±¡åŸºç±»
# ============================================================

class BaseLLMClient(ABC):
    """LLMå®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.http_client = httpx.Client(timeout=config.timeout)
    
    @abstractmethod
    def chat(self, 
             message: str, 
             system_prompt: Optional[str] = None,
             **kwargs) -> LLMResponse:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            LLMResponse
        """
        pass
    
    @abstractmethod
    def chat_with_history(self,
                          messages: List[Dict[str, str]],
                          system_prompt: Optional[str] = None,
                          **kwargs) -> LLMResponse:
        """
        å¸¦å†å²è®°å½•çš„èŠå¤©
        
        Args:
            messages: æ¶ˆæ¯å†å² [{"role": "user", "content": "..."}, ...]
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            LLMResponse
        """
        pass
    
    def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        self.http_client.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, *args):
        self.close()


# ============================================================
# OpenAI å®ç°
# ============================================================

class OpenAIClient(BaseLLMClient):
    """OpenAI APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.OPENAI]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.OPENAI])
        
        if not self.api_key:
            raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
    
    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ°OpenAI API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }
        
        response = self.http_client.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["choices"][0]["message"]["content"],
            model=data.get("model", self.config.model),
            usage=data.get("usage"),
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# Anthropic å®ç°
# ============================================================

class AnthropicClient(BaseLLMClient):
    """Anthropic Claude APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.ANTHROPIC]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.ANTHROPIC])
        
        if not self.api_key:
            raise ValueError("Anthropic API key not found. Set ANTHROPIC_API_KEY environment variable.")
    
    def _make_request(self, messages: List[Dict], system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ°Anthropic API"""
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        response = self.http_client.post(
            f"{self.api_base}/v1/messages",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["content"][0]["text"],
            model=data.get("model", self.config.model),
            usage={
                "prompt_tokens": data.get("usage", {}).get("input_tokens"),
                "completion_tokens": data.get("usage", {}).get("output_tokens")
            },
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = [{"role": "user", "content": message}]
        return self._make_request(messages, system_prompt, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        return self._make_request(messages, system_prompt, **kwargs)


# ============================================================
# é˜¿é‡Œäº‘é€šä¹‰åƒé—®å®ç°
# ============================================================

class AliyunClient(BaseLLMClient):
    """é˜¿é‡Œäº‘é€šä¹‰åƒé—®APIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.ALIYUN]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.ALIYUN])
        
        if not self.api_key:
            raise ValueError("Aliyun API key not found. Set DASHSCOPE_API_KEY environment variable.")
    
    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }
        
        response = self.http_client.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["choices"][0]["message"]["content"],
            model=data.get("model", self.config.model),
            usage=data.get("usage"),
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# æ™ºè°±GLMå®ç°
# ============================================================

class ZhipuClient(BaseLLMClient):
    """æ™ºè°±GLM APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.ZHIPU]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.ZHIPU])
        
        if not self.api_key:
            raise ValueError("Zhipu API key not found. Set ZHIPU_API_KEY environment variable.")
    
    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }
        
        response = self.http_client.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["choices"][0]["message"]["content"],
            model=data.get("model", self.config.model),
            usage=data.get("usage"),
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# DeepSeekå®ç°
# ============================================================

class DeepSeekClient(BaseLLMClient):
    """DeepSeek APIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.DEEPSEEK]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.DEEPSEEK])
        
        if not self.api_key:
            raise ValueError("DeepSeek API key not found. Set DEEPSEEK_API_KEY environment variable.")
    
    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }
        
        response = self.http_client.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["choices"][0]["message"]["content"],
            model=data.get("model", self.config.model),
            usage=data.get("usage"),
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# Ollamaæœ¬åœ°æ¨¡å‹å®ç°
# ============================================================

class OllamaClient(BaseLLMClient):
    """Ollamaæœ¬åœ°æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.OLLAMA]
    
    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ°Ollama"""
        payload = {
            "model": self.config.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", self.config.temperature),
                "num_predict": kwargs.get("max_tokens", self.config.max_tokens),
            }
        }
        
        response = self.http_client.post(
            f"{self.api_base}/api/chat",
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        
        return LLMResponse(
            content=data["message"]["content"],
            model=data.get("model", self.config.model),
            usage={
                "prompt_tokens": data.get("prompt_eval_count"),
                "completion_tokens": data.get("eval_count")
            },
            raw_response=data
        )
    
    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]], 
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# OpenAIå…¼å®¹æ¥å£ï¼ˆé€šç”¨ï¼‰
# ============================================================

class OpenAICompatibleClient(BaseLLMClient):
    """
    OpenAIå…¼å®¹æ¥å£å®¢æˆ·ç«¯
    
    é€‚ç”¨äºï¼š
    - vLLM
    - LocalAI
    - OneAPI
    - FastChat
    - å„ç§OpenAIä»£ç†æœåŠ¡
    """
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or os.getenv("LLM_API_BASE", "http://localhost:8000/v1")
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.OPENAI_COMPATIBLE], "sk-no-key-required")

    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚ï¼ˆå«è¯¦ç»†é”™è¯¯è®°å½•ï¼‰"""
        url = f"{self.api_base}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }

        # æå–promptæ‘˜è¦ç”¨äºé”™è¯¯æ—¥å¿—
        prompt_summary = ""
        if messages:
            last_msg = messages[-1].get("content", "")
            prompt_summary = last_msg[:100] + "..." if len(last_msg) > 100 else last_msg

        try:
            response = self.http_client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()

            # æå–content - å¢å¼ºå¯¹æ€è€ƒæ¨¡å‹çš„æ”¯æŒ
            message = data["choices"][0]["message"]
            content = message.get("content", "")
            reasoning_content = message.get("reasoning_content", "")

            # ğŸ†• æ™ºèƒ½åˆå¹¶ç­–ç•¥
            if not content and reasoning_content:
                # åªæœ‰reasoningï¼Œä½¿ç”¨å®ƒ
                content = reasoning_content
            elif content and reasoning_content:
                # ä¸¤è€…éƒ½æœ‰ï¼Œè¿›è¡Œæ™ºèƒ½åˆ¤æ–­
                content_stripped = content.strip()

                # å¦‚æœcontentæ˜¯çº¯JSONæ ¼å¼ï¼Œä¸åˆå¹¶reasoningï¼ˆä¿æŒçº¯å‡€ï¼‰
                if content_stripped.startswith('{') and content_stripped.endswith('}'):
                    pass  # ä¿æŒcontentä¸å˜
                # å¦‚æœreasoningä¸åœ¨contentä¸­ï¼Œåˆå¹¶å®ƒä»¬
                elif reasoning_content not in content:
                    content = f"{reasoning_content}\n\n{content}"

            return LLMResponse(
                content=content,
                model=data.get("model", self.config.model),
                usage=data.get("usage"),
                raw_response=data,
                reasoning_content=reasoning_content or None  # ğŸ†• ä¿ç•™åŸå§‹reasoning_content
            )

        except httpx.TimeoutException as e:
            error_msg = (
                f"LLMè¯·æ±‚è¶…æ—¶\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  è¶…æ—¶è®¾ç½®: {self.config.timeout}ç§’\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except httpx.HTTPStatusError as e:
            error_msg = (
                f"LLMè¯·æ±‚HTTPé”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  çŠ¶æ€ç : {e.response.status_code}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  å“åº”å†…å®¹: {e.response.text[:500] if e.response.text else 'N/A'}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except httpx.RequestError as e:
            error_msg = (
                f"LLMè¯·æ±‚ç½‘ç»œé”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except Exception as e:
            error_msg = (
                f"LLMè¯·æ±‚æœªçŸ¥é”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}\n"
                f"  å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
            )
            logger.error(error_msg)
            raise

    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)
    
    def chat_with_history(self, messages: List[Dict[str, str]],
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# ModelScopeå®¢æˆ·ç«¯
# ============================================================

class ModelScopeClient(BaseLLMClient):
    """
    ModelScope APIå®¢æˆ·ç«¯
    é˜¿é‡Œäº‘æ¨¡å‹æ‰˜ç®¡å¹³å°ï¼Œæ”¯æŒå¤šç§å¼€æºæ¨¡å‹

    æ–‡æ¡£: https://api-inference.modelscope.cn/docs
    """

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.api_base = config.api_base or API_BASES[LLMProvider.MODELSCOPE]
        self.api_key = config.api_key or os.getenv(ENV_API_KEYS[LLMProvider.MODELSCOPE])

        if not self.api_key:
            raise ValueError(
                "ModelScope API key not found. "
                "Please set MODELSCOPE_API_KEY environment variable or pass api_key parameter."
            )

    def _make_request(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ°ModelScope API"""
        url = f"{self.api_base}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.config.model,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "temperature": kwargs.get("temperature", self.config.temperature),
        }

        # æå–promptæ‘˜è¦ç”¨äºé”™è¯¯æ—¥å¿—
        prompt_summary = ""
        if messages:
            last_msg = messages[-1].get("content", "")
            prompt_summary = last_msg[:100] + "..." if len(last_msg) > 100 else last_msg

        try:
            response = self.http_client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()

            # æå–content - å¢å¼ºå¯¹æ€è€ƒæ¨¡å‹çš„æ”¯æŒ
            message = data["choices"][0]["message"]
            content = message.get("content", "")
            reasoning_content = message.get("reasoning_content", "")

            # ğŸ†• æ™ºèƒ½åˆå¹¶ç­–ç•¥
            if not content and reasoning_content:
                # åªæœ‰reasoningï¼Œä½¿ç”¨å®ƒ
                content = reasoning_content
            elif content and reasoning_content:
                # ä¸¤è€…éƒ½æœ‰ï¼Œè¿›è¡Œæ™ºèƒ½åˆ¤æ–­
                content_stripped = content.strip()

                # å¦‚æœcontentæ˜¯çº¯JSONæ ¼å¼ï¼Œä¸åˆå¹¶reasoningï¼ˆä¿æŒçº¯å‡€ï¼‰
                if content_stripped.startswith('{') and content_stripped.endswith('}'):
                    pass  # ä¿æŒcontentä¸å˜
                # å¦‚æœreasoningä¸åœ¨contentä¸­ï¼Œåˆå¹¶å®ƒä»¬
                elif reasoning_content not in content:
                    content = f"{reasoning_content}\n\n{content}"

            return LLMResponse(
                content=content,
                model=data.get("model", self.config.model),
                usage=data.get("usage"),
                raw_response=data,
                reasoning_content=reasoning_content or None  # ğŸ†• ä¿ç•™åŸå§‹reasoning_content
            )

        except httpx.TimeoutException as e:
            error_msg = (
                f"ModelScopeè¯·æ±‚è¶…æ—¶\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  è¶…æ—¶è®¾ç½®: {self.config.timeout}ç§’\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except httpx.HTTPStatusError as e:
            error_msg = (
                f"ModelScopeè¯·æ±‚HTTPé”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  çŠ¶æ€ç : {e.response.status_code}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  å“åº”å†…å®¹: {e.response.text[:500] if e.response.text else 'N/A'}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except httpx.RequestError as e:
            error_msg = (
                f"ModelScopeè¯·æ±‚ç½‘ç»œé”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}"
            )
            logger.error(error_msg)
            raise

        except Exception as e:
            error_msg = (
                f"ModelScopeè¯·æ±‚æœªçŸ¥é”™è¯¯\n"
                f"  é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"  é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"  è¯·æ±‚URL: {url}\n"
                f"  è¯·æ±‚æ¨¡å‹: {self.config.model}\n"
                f"  Promptæ‘˜è¦: {prompt_summary}\n"
                f"  å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
            )
            logger.error(error_msg)
            raise

    def chat(self, message: str, system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        return self._make_request(messages, **kwargs)

    def chat_with_history(self, messages: List[Dict[str, str]],
                          system_prompt: Optional[str] = None, **kwargs) -> LLMResponse:
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        return self._make_request(full_messages, **kwargs)


# ============================================================
# å·¥å‚å‡½æ•°
# ============================================================

# å®¢æˆ·ç«¯ç±»æ˜ å°„
CLIENT_MAP = {
    LLMProvider.OPENAI: OpenAIClient,
    LLMProvider.ANTHROPIC: AnthropicClient,
    LLMProvider.ALIYUN: AliyunClient,
    LLMProvider.ZHIPU: ZhipuClient,
    LLMProvider.DEEPSEEK: DeepSeekClient,
    LLMProvider.OLLAMA: OllamaClient,
    LLMProvider.OPENAI_COMPATIBLE: OpenAICompatibleClient,
    LLMProvider.MODELSCOPE: ModelScopeClient,
}


def create_llm_client(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    api_base: Optional[str] = None,
    **kwargs
) -> BaseLLMClient:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯
    
    Args:
        provider: æä¾›å•†åç§°ï¼ˆopenai/anthropic/aliyun/zhipu/deepseek/ollama/openai_compatibleï¼‰
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
        api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        api_base: APIåœ°å€ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤åœ°å€ï¼‰
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        BaseLLMClientå®ä¾‹
        
    Examples:
        # ä½¿ç”¨OpenAI
        client = create_llm_client(provider="openai", model="gpt-4o")
        
        # ä½¿ç”¨é€šä¹‰åƒé—®
        client = create_llm_client(provider="aliyun", model="qwen-max")
        
        # ä½¿ç”¨æœ¬åœ°Ollama
        client = create_llm_client(provider="ollama", model="llama3.1:70b")
        
        # ä½¿ç”¨è‡ªå®šä¹‰OpenAIå…¼å®¹æ¥å£
        client = create_llm_client(
            provider="openai_compatible",
            api_base="http://my-server:8000/v1",
            model="my-model"
        )
        
        # è‡ªåŠ¨æ£€æµ‹ï¼ˆæ ¹æ®ç¯å¢ƒå˜é‡ï¼‰
        client = create_llm_client()
    """
    
    # è‡ªåŠ¨æ£€æµ‹æä¾›å•†
    if provider is None:
        provider = _detect_provider()
    
    # è§£ææä¾›å•†æšä¸¾
    if isinstance(provider, str):
        try:
            provider_enum = LLMProvider(provider.lower())
        except ValueError:
            raise ValueError(f"Unknown provider: {provider}. Supported: {[p.value for p in LLMProvider]}")
    else:
        provider_enum = provider
    
    # è·å–é»˜è®¤æ¨¡å‹
    if model is None:
        model = DEFAULT_MODELS.get(provider_enum, "gpt-4o")
    
    # åˆ›å»ºé…ç½®
    config = LLMConfig(
        provider=provider_enum,
        model=model,
        api_key=api_key,
        api_base=api_base,
        max_tokens=kwargs.get("max_tokens", 2000),
        temperature=kwargs.get("temperature", 0.7),
        timeout=kwargs.get("timeout", 60),
        extra_params=kwargs.get("extra_params"),
    )
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client_class = CLIENT_MAP.get(provider_enum)
    if client_class is None:
        raise ValueError(f"No client implementation for provider: {provider_enum}")
    
    return client_class(config)


def _detect_provider() -> str:
    """æ ¹æ®ç¯å¢ƒå˜é‡è‡ªåŠ¨æ£€æµ‹æä¾›å•†"""

    # æŒ‰ä¼˜å…ˆçº§æ£€æµ‹
    detection_order = [
        (ENV_API_KEYS[LLMProvider.OPENAI], "openai"),
        (ENV_API_KEYS[LLMProvider.ANTHROPIC], "anthropic"),
        (ENV_API_KEYS[LLMProvider.DEEPSEEK], "deepseek"),
        (ENV_API_KEYS[LLMProvider.MODELSCOPE], "modelscope"),
        (ENV_API_KEYS[LLMProvider.ALIYUN], "aliyun"),
        (ENV_API_KEYS[LLMProvider.ZHIPU], "zhipu"),
    ]

    for env_var, provider in detection_order:
        if os.getenv(env_var):
            return provider

    # æ£€æŸ¥æ˜¯å¦æœ‰Ollamaåœ¨è¿è¡Œ
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            return "ollama"
    except:
        pass

    # æ£€æŸ¥é€šç”¨LLMé…ç½®
    if os.getenv("LLM_API_BASE"):
        return "openai_compatible"

    raise ValueError(
        "No LLM provider detected. Please set one of the following environment variables:\n"
        "  - OPENAI_API_KEY (for OpenAI)\n"
        "  - ANTHROPIC_API_KEY (for Claude)\n"
        "  - DEEPSEEK_API_KEY (for DeepSeek)\n"
        "  - MODELSCOPE_API_KEY (for ModelScope)\n"
        "  - DASHSCOPE_API_KEY (for Aliyun/Qwen)\n"
        "  - ZHIPU_API_KEY (for Zhipu/GLM)\n"
        "  - Or start Ollama locally\n"
        "  - Or set LLM_API_BASE for OpenAI-compatible endpoint"
    )


def list_available_providers() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æä¾›å•†"""
    return [p.value for p in LLMProvider]


def get_provider_info(provider: str) -> Dict[str, Any]:
    """è·å–æä¾›å•†ä¿¡æ¯"""
    try:
        p = LLMProvider(provider.lower())
        return {
            "name": p.value,
            "default_model": DEFAULT_MODELS.get(p),
            "api_base": API_BASES.get(p),
            "env_var": ENV_API_KEYS.get(p),
        }
    except ValueError:
        return None


# ============================================================
# æ€è€ƒæ¨¡å‹è¯†åˆ«
# ============================================================

def is_reasoning_model(model_name: str) -> bool:
    """
    åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹ï¼ˆæ¨ç†æ¨¡å‹ï¼‰

    æ€è€ƒæ¨¡å‹é€šå¸¸å…·æœ‰æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡å¦‚Tagåˆ†ç±»ã€‚
    å¿«é€Ÿæ¨¡å‹åˆ™æ›´é€‚åˆç®€å•ä»»åŠ¡å¦‚ç­–ç•¥æ‰«æã€‚

    Args:
        model_name: æ¨¡å‹åç§°

    Returns:
        æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹

    è¯†åˆ«è§„åˆ™ï¼š
    1. åŒ…å« "reasoner" (ä¸åŒºåˆ†å¤§å°å†™) - DeepSeekæ¨ç†æ¨¡å‹
    2. åŒ…å« "-R1" æˆ– ":R1" (ä¸åŒºåˆ†å¤§å°å†™) - DeepSeek R1ç³»åˆ—
    3. ä»¥ "o1" å¼€å¤´ (ä¸åŒºåˆ†å¤§å°å†™) - OpenAI o1æ¨ç†ç³»åˆ—

    Examples:
        >>> is_reasoning_model("deepseek-reasoner")
        True
        >>> is_reasoning_model("deepseek-ai/DeepSeek-R1")
        True
        >>> is_reasoning_model("deepseek-r1:7b")
        True
        >>> is_reasoning_model("o1-preview")
        True
        >>> is_reasoning_model("deepseek-chat")
        False
        >>> is_reasoning_model("gpt-4o")
        False
    """
    if not model_name:
        return False

    model_lower = model_name.lower()

    # æ€è€ƒæ¨¡å‹è¯†åˆ«æ¨¡å¼
    reasoning_patterns = [
        'reasoner',    # DeepSeek Reasoner
        '-r1',         # DeepSeek R1 (URLæ ¼å¼)
        ':r1',         # DeepSeek R1 (Ollamaæ ¼å¼)
        'o1',          # OpenAI o1ç³»åˆ—
    ]

    return any(pattern in model_lower for pattern in reasoning_patterns)


def get_model_display_name(model: str, show_marker: bool = True) -> str:
    """
    è·å–æ¨¡å‹çš„æ˜¾ç¤ºåç§°ï¼ˆå«æ€è€ƒæ¨¡å‹æ ‡è®°ï¼‰

    Args:
        model: æ¨¡å‹åç§°
        show_marker: æ˜¯å¦æ˜¾ç¤ºç±»å‹æ ‡è®°

    Returns:
        å¸¦æ ‡è®°çš„æ˜¾ç¤ºåç§°ï¼Œå¦‚ "deepseek-chat [FAST]" æˆ– "DeepSeek-R1 [THINK]"

    Examples:
        >>> get_model_display_name("deepseek-chat")
        'deepseek-chat [FAST]'
        >>> get_model_display_name("deepseek-reasoner")
        'deepseek-reasoner [THINK]'
        >>> get_model_display_name("DeepSeek-V3", show_marker=False)
        'DeepSeek-V3'
    """
    if not show_marker:
        return model

    if is_reasoning_model(model):
        return f"{model} [THINK]"
    return f"{model} [FAST]"


def get_model_icon(model: str) -> str:
    """
    è·å–æ¨¡å‹çš„å›¾æ ‡

    Args:
        model: æ¨¡å‹åç§°

    Returns:
        å›¾æ ‡å­—ç¬¦ä¸²ï¼šæ€è€ƒæ¨¡å‹è¿”å› "ğŸ§ª"ï¼Œå¿«é€Ÿæ¨¡å‹è¿”å› "âš¡"
    """
    return "ğŸ§ª" if is_reasoning_model(model) else "âš¡"


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================

def quick_chat(message: str, 
               provider: Optional[str] = None,
               model: Optional[str] = None,
               system_prompt: Optional[str] = None) -> str:
    """
    å¿«é€Ÿå‘é€èŠå¤©è¯·æ±‚
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        provider: æä¾›å•†ï¼ˆå¯é€‰ï¼‰
        model: æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        åŠ©æ‰‹å›å¤å†…å®¹
        
    Example:
        response = quick_chat("ä»€ä¹ˆæ˜¯å¥—åˆ©ï¼Ÿ", provider="deepseek")
    """
    with create_llm_client(provider=provider, model=model) as client:
        response = client.chat(message, system_prompt=system_prompt)
        return response.content


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================

